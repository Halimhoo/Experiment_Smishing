{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d448a2",
   "metadata": {},
   "source": [
    "# Project Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f03c82",
   "metadata": {},
   "source": [
    "\n",
    "DISCLAIMER: STILL WORK ON PROGRESS - VERY MESSY CODE. THANKS\n",
    "\n",
    "__Establishing Dataset__\n",
    "\n",
    "First Dataset from\n",
    "\n",
    "    https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset\n",
    "\n",
    "Second Taken from \n",
    "\n",
    "    https://data.mendeley.com/datasets/f45bkkt8pr/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c53366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#requirements - if error install this\n",
    "#!pip install pandas \n",
    "#!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ce42f",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a21575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing\n",
    "import pandas as pd\n",
    "\n",
    "#SINCE I HAVE PROBLEM IN COMBINING, I STILL EXPERIMENT THE DATASET ONE BY ONE\n",
    "#so here is the set up, add your database directory here\n",
    "#First Dataset\n",
    "first_dir = r\"C:\\Users\\user\\Desktop\\Current Task\\SIT746\\Project\\spam.csv\"\n",
    "#Second Dataset\n",
    "second_dir = r\"C:\\Users\\user\\Desktop\\Current Task\\SIT746\\Project\\Dataset_5971\\Dataset_5971.csv\"\n",
    "\n",
    "############################################\n",
    "#select database here - CHOOSE 'first_dir' OR 'second_dir' HERE\n",
    "selected_data = first_dir\n",
    "############################################\n",
    "\n",
    "#STILL MESSY AND MANUAL. PLEASE BE PATIENT \n",
    "if selected_data == first_dir:\n",
    "    file_path = first_dir\n",
    "    label_column = \"v1\"\n",
    "    text_column = \"v2\"\n",
    "    \n",
    "else:\n",
    "    file_path = second_dir\n",
    "    label_column = \"LABEL\"\n",
    "    text_column = \"TEXT\"\n",
    "\n",
    "#reading the selected dataset\n",
    "dataset = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da3eb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will �_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will �_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#showing dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a9a7d",
   "metadata": {},
   "source": [
    "## Preposessing Phase\n",
    "\n",
    "   __Status Progression__  \n",
    "    Lowercase - __DONE__<BR>\n",
    "    Spelling  -  __GOT PROBLEM__ - many words cannot be corrected\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea98c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Operation Lower Case\n",
    "\n",
    "#testing phase\n",
    "def lowercase_testing():\n",
    "    test_text = \"This IS a STRESS TEST to for LOWERCASING words\"\n",
    "    test_text = test_text.lower()\n",
    "    print(test_text)\n",
    "\n",
    "#test below to see if its working.\n",
    "#lowercase_testing()\n",
    "\n",
    "#################################################\n",
    "\n",
    "#the operation begin\n",
    "#column = 'TEXT' #we just want to lower the column dataset\n",
    "\n",
    "def lowercase_testing(column):\n",
    "    dataset[column] = dataset[column].str.lower()\n",
    "\n",
    "### Lowercase Operation is DONE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f7c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Operation Spelling Correction\n",
    "\n",
    "#importing libraries\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "#testing phase\n",
    "def spelling_correction_testing():\n",
    "    \n",
    "    test = \"my name is kusruthi \" #experiment the words \n",
    "    \n",
    "    split_words = test.split() #splitting the words \n",
    "    correction_words = [] #make place to store the words\n",
    "    \n",
    "    #sometimes there is word that no need for correction.\n",
    "    for word in split_words:\n",
    "        correction = SpellChecker().correction(word)\n",
    "        if correction is None:\n",
    "            correction_words.append(word)\n",
    "        else:\n",
    "            correction_words.append(correction)\n",
    "            \n",
    "    #collect the words\n",
    "    final_text = ' '.join(correction_words)\n",
    "    print(final_text)\n",
    "\n",
    "\n",
    "#NOTE\n",
    "#from the test, there are some text is not properly corrected, but some do. good enough lmao\n",
    "#I dont know if I need to maintain this preprocessing or not but yeah.\n",
    "\n",
    "#################################################\n",
    "\n",
    "#operation begin\n",
    "#since the there are many iteration, need to make a function for each line so here we go.\n",
    "def spelling_correction(text):\n",
    "    split_words = text.split() #splitting the text into words\n",
    "    correction_words = [] #make place to store the words\n",
    "    \n",
    "    for word in split_words:\n",
    "        correction = SpellChecker().correction(word)\n",
    "        if correction is None:\n",
    "            correction_words.append(word)\n",
    "        else:\n",
    "            correction_words.append(correction)\n",
    "\n",
    "    correction_final = ' '.join(correction_words)\n",
    "    return correction_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe16271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONGRATULATION Preprocessing Success\n"
     ]
    }
   ],
   "source": [
    "#Here are the part where we initiate the prepocessing part. To make easier to pick which one we want to use.\n",
    "\n",
    "\n",
    "#lowercase Operation - GOOD\n",
    "lowercase_testing(text_column) #- We want to lowercase the columnt text only\n",
    "\n",
    "#Spelling Correction Operation - DO NOT USE THIS, MORE\n",
    "#dataset['TEXT'] = dataset['TEXT'].apply(spelling_correction)\n",
    "\n",
    "\n",
    "#Preprocessing is done\n",
    "print('CONGRATULATION Preprocessing Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d6fbb",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d8ecedb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "#Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "def BOW_Extraction(): # overall have better results\n",
    "    #create countvectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    #Operation begin\n",
    "    # Fit and transform the 'text' column into BoW representation\n",
    "    Extraction = vectorizer.fit_transform(dataset[text_column])\n",
    "    return Extraction \n",
    "\n",
    "############################################################\n",
    "\n",
    "#TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def TFID():\n",
    "    #create vectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    \n",
    "    #create the data\n",
    "    Extraction = vectorizer.fit_transform(dataset[text_column])\n",
    "    return Extraction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c99bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGING extraction method here\n",
    "#Extraction = BOW_Extraction()\n",
    "Extraction = TFID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a9164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c9cb60c",
   "metadata": {},
   "source": [
    "## Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "220f421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Splitting Begin 80:20\n",
    "X_train, X_test, y_train, y_test = train_test_split(Extraction, dataset[label_column], test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fec60c2",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c27e5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Training multiple classifier\n",
    "classifiers = {MultinomialNB(),\n",
    "              SVC(),\n",
    "              DecisionTreeClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "647c4c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM STILL IN PROGRESS - it have different way in learning progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f344e4",
   "metadata": {},
   "source": [
    "## Model Train and Evaluates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ce11d06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE EVALUATION OF SVC()\n",
      "Accuracy:\t 0.9802690582959641\n",
      "Precision:\t 0.9802690582959641\n",
      "Recall:\t\t 0.9802690582959641\n",
      "F1-score:\t 0.9802690582959641\n",
      "---------------------\n",
      "\n",
      "THE EVALUATION OF DecisionTreeClassifier()\n",
      "Accuracy:\t 0.9695067264573991\n",
      "Precision:\t 0.9695067264573991\n",
      "Recall:\t\t 0.9695067264573991\n",
      "F1-score:\t 0.9695067264573991\n",
      "---------------------\n",
      "\n",
      "THE EVALUATION OF MultinomialNB()\n",
      "Accuracy:\t 0.9748878923766816\n",
      "Precision:\t 0.9748878923766816\n",
      "Recall:\t\t 0.9748878923766816\n",
      "F1-score:\t 0.9748878923766816\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#train and testing\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    #evaluates\n",
    "    #Calculating the results\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='micro')\n",
    "    recall = recall_score(y_test, y_pred, average='micro')\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    #Printing the results\n",
    "    print(\"THE EVALUATION OF \" + str(classifier))\n",
    "    print(\"Accuracy:\\t\", accuracy)\n",
    "    print(\"Precision:\\t\", precision)\n",
    "    print(\"Recall:\\t\\t\", recall)\n",
    "    print(\"F1-score:\\t\", f1)\n",
    "    print(\"---------------------\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9d740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7ea4bcf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb80c7c",
   "metadata": {},
   "source": [
    "### Reference\n",
    "mishra, sandhya; Soni, Devpriya (2022), “SMS PHISHING DATASET FOR MACHINE LEARNING AND PATTERN RECOGNITION”, Mendeley Data, V1, doi: 10.17632/f45bkkt8pr.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
